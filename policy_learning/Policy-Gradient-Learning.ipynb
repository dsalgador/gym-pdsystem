{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dsalgador/anaconda3/envs/tensorflow/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "The algorithm is tested on the PDSystemEnv  gym task \n",
    "and developed with Tensorflow\n",
    "\n",
    "Author: Daniel Salgado Rojo\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gym\n",
    "import gym_pdsystem\n",
    "from gym import wrappers\n",
    "#import tflearn\n",
    "import argparse\n",
    "import pprint as pp\n",
    "\n",
    "#from ddpg.replay_buffer import ReplayBuffer\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import random\n",
    "from gym_pdsystem.envs.pdsystem_env import PDSystemEnv\n",
    "\n",
    "import gym_pdsystem.utils.utilsq as ut\n",
    "import gym_pdsystem.utils.constants as ct\n",
    "\n",
    "\n",
    "from matplotlib import animation, rc\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML\n",
    "\n",
    "\n",
    "#TO OMMIT WARNINGS\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# Example n=5, k = 2\n",
    "\n",
    "TANK_MAX_LOADS = np.array([100., 200, 100., 800., 200.])\n",
    "LEVEL_PERCENTAGES = np.array([ #b , c, e\n",
    "                                                [0.02, 0.31, 0.9],\n",
    "                                                [0.01, 0.03, 0.9],\n",
    "                                                [0.05, 0.16, 0.9],\n",
    "                                                [0.07, 0.14, 0.85],\n",
    "                                                [0.08, 0.26, 0.9]\n",
    "                                                   ])\n",
    "\n",
    "TRUCK_MAX_LOADS = np.array([70.,130.])\n",
    "\n",
    "GRAPH_WEIGHTS = np.array([32., 159., 162., 156.,156., 0.])\n",
    "DISCRETE = True\n",
    "############################################################\n",
    "\n",
    "env = gym.make(\"PDSystemEnv-v0\")\n",
    "episode_length = 30\n",
    "env._max_episode_steps = episode_length\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def int_to_action(int_action: int, env):\n",
    "    \"\"\"\n",
    "    So far assumed k = 2:\n",
    "    \n",
    "    Converts an integer between 0 and env.action_space.shape[1]**env.action_space.shape[0]\n",
    "    which is (n+1)^k where n is the number of tanks and k the number of trucks.\n",
    "    \n",
    "    return vect_action: a k-dimensional vector with components in the range 0,...n. \n",
    "    For k = 2, vect_action = [i,j] is the action of truck 1 going to tank i and truck 2 going to tank j.\n",
    "    (i, j = n means staying at the depot, 0,....,n-1 are the real tanks).\n",
    "    The associated integer is i*(n+1) + j\n",
    "    \"\"\"\n",
    "    nplus1 = env.action_space.shape[1]\n",
    "    k = env.action_space.shape[0]\n",
    "    n_actions = nplus1**k\n",
    "    \n",
    "    j = int_action % nplus1\n",
    "    i = int((int_action-j)/nplus1)\n",
    "    vect_action = np.array([i,j])\n",
    "    return vect_action\n",
    "\n",
    "def action_to_int(vect_action: np.array, env):\n",
    "    \"\"\"\n",
    "    Assumed k = 2, so vect_action has 2 components\n",
    "    \"\"\"\n",
    "    nplus1 = env.action_space.shape[1]\n",
    "    int_action = vect_action[0] * nplus1 + vect_action[1]\n",
    "    return int_action\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "[2 2]\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "int_action = 14\n",
    "print(int_action)\n",
    "vect_action = int_to_action(int_action,env)\n",
    "print(vect_action)\n",
    "int_action = action_to_int(vect_action,env)\n",
    "print(int_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/dsalgador/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "seed = 42\n",
    "learning_rate = 0.01 #0.01\n",
    "\n",
    "model_file = './final_pgmodel.ckpt'#.format(learning_rate)\n",
    "graph_file =  '{}.meta'.format(model_file)\n",
    "epochs = 100 #2000\n",
    "batch_size = 50 #50\n",
    "\n",
    "summary_freq = 100#np.ceil(epochs/10) #200\n",
    "\n",
    "\n",
    "hidden1_neurons = 100 #100\n",
    "hidden2_neurons = 50 #50\n",
    "\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "            #inputs = tflearn.input_data(shape=[None, self.s_dim])\n",
    "            tf.set_random_seed(seed)\n",
    "            # 1. Parameters to determine the NN architecture\n",
    "\n",
    "            n_inputs = env.observation_space.shape[1]\n",
    "            n_hidden1 = hidden1_neurons; activation1 = tf.nn.sigmoid#tf.nn.elu\n",
    "            n_hidden2 = hidden2_neurons; activation2 = tf.nn.sigmoid#tf.nn.elu\n",
    "            n_outputs = env.action_space.shape[1]**env.action_space.shape[0]\n",
    "            \n",
    "            #indices = [i for i in range(n_outputs)]\n",
    "            \n",
    "            initializer = tf.contrib.layers.variance_scaling_initializer()\n",
    "\n",
    "            # 2. Build the Neural Network\n",
    "            \n",
    "            X = tf.placeholder(tf.float32, shape = (None, n_inputs), name = \"X\")\n",
    "            #y = tf.placeholder(tf.int64, shape = (None), name = \"y\")\n",
    "            \n",
    "            hidden1 = tf.layers.dense(X, n_hidden1, activation = activation1,\n",
    "                                     kernel_initializer = initializer)\n",
    "            hidden2 = tf.layers.dense(hidden1, n_hidden2, activation = activation2,\n",
    "                                     kernel_initializer = initializer)\n",
    "            logits = tf.layers.dense(hidden2, n_outputs)#,kernel_initializer = initializer)\n",
    "            outputs = tf.nn.softmax(logits)\n",
    "                   \n",
    "            \n",
    "with tf.name_scope(\"action\"):\n",
    "            # 3. Select a random action (where to go) based on the estimated probabilities\n",
    "            action = tf.multinomial(tf.log(outputs), num_samples = 1)\n",
    "            #print(tf.rank(action))\n",
    "            #action_onehot = tf.one_hot(indices, n_outputs)\n",
    "            #y = tf.reshape(action_onehot[action], (n_outputs, None))\n",
    "            #y = tf.Variable(action, tf.int64)\n",
    "            y = tf.reshape(action, [1])\n",
    "            #print(tf.rank(y))\n",
    "            #print(tf.rank(logits))\n",
    "            \n",
    "with tf.name_scope(\"loss\"):\n",
    "            xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y,logits = logits)\n",
    "#             xentropy = tf.nn.sigmoid_cross_entropy_with_logits(labels = y,\n",
    "#                                                                       logits = logits)\n",
    "            loss = tf.reduce_mean(xentropy, name = \"loss\")\n",
    "            \n",
    "tf.summary.scalar('average_cross_entropy', loss)\n",
    "\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "            # Optimization Op\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "            #optimize = optimizer.minimize(loss)\n",
    "            \n",
    "            grads_and_vars = optimizer.compute_gradients(xentropy)\n",
    "            gradients = [grad for grad, variable in grads_and_vars]\n",
    "            gradient_placeholders = []\n",
    "            grads_and_vars_feed = []\n",
    "            for grad, variable in grads_and_vars:\n",
    "                gradient_placeholder = tf.placeholder(tf.float32, shape=grad.get_shape())\n",
    "                gradient_placeholders.append(gradient_placeholder)\n",
    "                grads_and_vars_feed.append((gradient_placeholder, variable))\n",
    "            training_op = optimizer.apply_gradients(grads_and_vars_feed)\n",
    "\n",
    "                        \n",
    "# # with tf.name_scope(\"eval\"):\n",
    "# #             correct = tf.nn.in_top_k(logits, y, 1)\n",
    "# #             accuracy = tf.reduce_mean(tf.cast(correct,tf.float32))\n",
    "# # tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "                       \n",
    "merged = tf.summary.merge_all()\n",
    "init = tf.global_variables_initializer()\n",
    "        \n",
    "if model_file != None:\n",
    "            saver = tf.train.Saver()\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "From https://github.com/ageron/handson-ml \n",
    "\"\"\"\n",
    "\n",
    "def discount_rewards(rewards, discount_rate):\n",
    "    discounted_rewards = np.zeros(len(rewards))\n",
    "    cumulative_rewards = 0\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        cumulative_rewards = rewards[step] + cumulative_rewards * discount_rate\n",
    "        discounted_rewards[step] = cumulative_rewards\n",
    "    return discounted_rewards\n",
    "\n",
    "def discount_and_normalize_rewards(all_rewards, discount_rate):\n",
    "    all_discounted_rewards = [discount_rewards(rewards, discount_rate) for rewards in all_rewards]\n",
    "    flat_rewards = np.concatenate(all_discounted_rewards)\n",
    "    reward_mean = flat_rewards.mean()\n",
    "    reward_std = flat_rewards.std()\n",
    "    return [(discounted_rewards - reward_mean)/reward_std for discounted_rewards in all_discounted_rewards]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0Average reward per game:  -23.707076144714296\n",
      "Iteration: 100Average reward per game:  -4.617355998994467\n",
      "Iteration: 200Average reward per game:  -3.8174857951707444\n",
      "Iteration: 300Average reward per game:  -0.872815612824542\n",
      "Iteration: 400Average reward per game:  -0.2636985066770518\n",
      "Iteration: 500Average reward per game:  -0.7136937584870859\n",
      "Iteration: 600Average reward per game:  0.9337049187302673\n",
      "Iteration: 700Average reward per game:  -0.3214531851295282\n",
      "Iteration: 800Average reward per game:  0.0016459132849497848\n",
      "Iteration: 900Average reward per game:  0.8573764411178175\n",
      "Iteration: 1000Average reward per game:  0.5041416907647117\n",
      "Iteration: 1100Average reward per game:  -0.30776797034858794\n",
      "Iteration: 1200Average reward per game:  0.7071059892980245\n",
      "Iteration: 1300Average reward per game:  0.3979504619078414\n",
      "Iteration: 1400Average reward per game:  0.39320993772550433\n",
      "Iteration: 1500Average reward per game:  0.523555564607407\n",
      "Iteration: 1600Average reward per game:  -0.22839775710557433\n",
      "Iteration: 1700Average reward per game:  0.23560141720580158\n",
      "Iteration: 1800Average reward per game:  -0.6389919833572727\n",
      "Iteration: 1900Average reward per game:  0.09875054272402163\n",
      "Iteration: 2000Average reward per game:  -0.07623849443822088\n",
      "Iteration: 2100Average reward per game:  0.8504931920801105\n",
      "Iteration: 2200Average reward per game:  0.9688893671988822\n",
      "Iteration: 2300Average reward per game:  0.7634418620238576\n",
      "Iteration: 2400Average reward per game:  0.32728032594642953\n",
      "Iteration: 2500Average reward per game:  0.4044640262720229\n",
      "Iteration: 2600Average reward per game:  0.5917661360260107\n",
      "Iteration: 2700Average reward per game:  0.5850912643155873\n",
      "Iteration: 2800Average reward per game:  0.8203159723202906\n",
      "Iteration: 2900Average reward per game:  1.0078654212695635\n",
      "Iteration: 3000Average reward per game:  -0.14772628047180356\n",
      "Iteration: 3100Average reward per game:  -0.6177016661752986\n",
      "Iteration: 3200Average reward per game:  -1.5387335036161658\n",
      "Iteration: 3300Average reward per game:  0.6379866356758672\n",
      "Iteration: 3400Average reward per game:  0.7743116077814202\n",
      "Iteration: 3500Average reward per game:  0.22930557595302456\n",
      "Iteration: 3600Average reward per game:  1.1664483891846837\n",
      "Iteration: 3700Average reward per game:  0.12659980109398647\n",
      "Iteration: 3800Average reward per game:  0.6773344806341981\n",
      "Iteration: 3900Average reward per game:  0.3463792124465131\n",
      "Iteration: 4000Average reward per game:  0.09783191840798508\n",
      "Iteration: 4099"
     ]
    }
   ],
   "source": [
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "\n",
    "best_acc_val = 0\n",
    "acc_val = 0\n",
    "\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir, now)   \n",
    "\n",
    "n_games_per_update = 10\n",
    "n_max_steps = episode_length\n",
    "n_iterations = 10000\n",
    "save_iterations = 500\n",
    "discount_rate = 0.95\n",
    "\n",
    "summary_freq = 100\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    #train_writer = tf.summary.FileWriter(logdir + '/pgtrain', sess.graph)\n",
    "\n",
    "    init.run()\n",
    "    for iteration in range(n_iterations+1):\n",
    "        print(\"\\rIteration: {}\".format(iteration), end=\"\")\n",
    "        all_rewards = []\n",
    "        all_gradients = []\n",
    "        for game in range(n_games_per_update):\n",
    "            current_rewards = []\n",
    "            current_gradients = []\n",
    "            obs = env.reset()\n",
    "            for step in range(n_max_steps):\n",
    "                action_val, gradients_val = sess.run([action, gradients], feed_dict={X: obs.reshape(1, n_inputs)}) \n",
    "                vect_action = int_to_action(action_val,env) #HERE WE CONVERT FROM INTEGER TO ACTION's Array\n",
    "                obs, reward, done, info = env.step(vect_action)\n",
    "                current_rewards.append(reward)\n",
    "                current_gradients.append(gradients_val)\n",
    "                \n",
    "#                 error = sess.run(loss, feed_dict={                                        \n",
    "#                                         X: obs.reshape(1, n_inputs)\n",
    "#                                     })\n",
    "                #print(\"Loss: \", \"Reward: \", reward)\n",
    "                #train_writer.add_summary(summary, iteration)\n",
    "                \n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "            all_rewards.append(current_rewards)\n",
    "            all_gradients.append(current_gradients)\n",
    "            \n",
    "        if iteration % summary_freq == 0:  \n",
    "            avg_rewards = np.mean(np.array(all_rewards))\n",
    "            print(\"Average reward per game: \",  avg_rewards / n_games_per_update)    \n",
    "\n",
    "        all_rewards = discount_and_normalize_rewards(all_rewards, discount_rate=discount_rate)\n",
    "        feed_dict = {}\n",
    "        for var_index, gradient_placeholder in enumerate(gradient_placeholders):\n",
    "            mean_gradients = np.mean([reward * all_gradients[game_index][step][var_index]\n",
    "                                      for game_index, rewards in enumerate(all_rewards)\n",
    "                                          for step, reward in enumerate(rewards)], axis=0)\n",
    "            feed_dict[gradient_placeholder] = mean_gradients\n",
    "         \n",
    "       \n",
    "        #tf.summary.scalar('avg_rewards', avg_rewards)\n",
    "\n",
    "\n",
    "        #train_writer.add_summary(avg_rewards, iteration)\n",
    "\n",
    "\n",
    "        sess.run(training_op, feed_dict=feed_dict)\n",
    "        if iteration % save_iterations == 0:\n",
    "                  saver.save(sess, \"./pdenv_policy_net_pg.ckpt\")\n",
    "    #train_writer.close()\n",
    "# with tf.Session() as sess:\n",
    "#         train_writer = tf.summary.FileWriter(log_dir + '/train', sess.graph)\n",
    "#         val_writer = tf.summary.FileWriter(log_dir + '/val')\n",
    "        \n",
    "#         init.run()\n",
    "#         N_TRAIN = X_train.shape[0]\n",
    "#         m = y_train.shape[0]\n",
    "        \n",
    "#         n_batches = int(np.ceil(N_TRAIN/ batch_size))\n",
    "#         #print(n_batches)\n",
    "             \n",
    "#         for epoch in range(epochs+1):\n",
    "            \n",
    "#             for batch_index in range(n_batches):\n",
    "#                 X_batch, y_batch = fetch_batch(X_train,y_train,epoch, batch_index, batch_size, m, n_batches)\n",
    "#                 #print(\"Xbatch shape\",X_batch.shape, \"y_batch shape\", y_batch.shape)\n",
    "#                 sess.run(optimize, feed_dict={\n",
    "#                                     y: y_batch,\n",
    "#                                     X: X_batch\n",
    "#                                 })\n",
    "    \n",
    "#             if summary_freq != None: \n",
    "                            \n",
    "#                 if epoch % summary_freq  == 0:\n",
    "                    \n",
    "\n",
    "#                     summary, acc_train = sess.run([merged, accuracy], feed_dict={\n",
    "#                                         y: y_batch,\n",
    "#                                         X: X_batch\n",
    "#                                     })\n",
    "#                     train_writer.add_summary(summary, epoch)\n",
    "\n",
    "#                     summary, acc_val = sess.run([merged, accuracy], feed_dict={\n",
    "#                                         y: y_val,\n",
    "#                                         X: X_val\n",
    "#                                     }) \n",
    "                                    \n",
    "#                     val_writer.add_summary(summary, epoch)\n",
    "\n",
    "#                     best_acc_val = max(best_acc_val, acc_val)\n",
    "#                     #print(acc_val, best_acc_val)\n",
    "\n",
    "#                     print(\"Epoch: \", epoch, \" Train (batch) accuracy: \", acc_train, \n",
    "#                           \" Validation accuracy: \", acc_val)\n",
    "#                     if best_acc_val <= acc_val:                        \n",
    "#                         save_path = saver.save(sess, model_file)\n",
    "#                         print(\"Saved model with validation accuracy \", acc_val)\n",
    "#         train_writer.close()\n",
    "#         val_writer.close()                            \n",
    "#         #save_path = saver.save(sess, model_file)   \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Iteration: 0Average reward per game:  -131.21281576006612\n",
    "Iteration: 100Average reward per game:  -92.23713538883702\n",
    "Iteration: 200Average reward per game:  -70.92879872240424\n",
    "Iteration: 300Average reward per game:  -46.36746628903998\n",
    "Iteration: 400Average reward per game:  -43.44814607253152\n",
    "Iteration: 500Average reward per game:  -26.28340975064497\n",
    "Iteration: 600Average reward per game:  -24.65947230415822\n",
    "Iteration: 700Average reward per game:  -30.107651383652463\n",
    "Iteration: 800Average reward per game:  -17.534408167806813\n",
    "Iteration: 900Average reward per game:  -19.252239544660405\n",
    "Iteration: 1000Average reward per game:  -20.431260399119516"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PLAY:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "\n",
    "frames = []\n",
    "n_episodes = 10\n",
    "\n",
    "system = PDSystemEnv()\n",
    "\n",
    "model_file = \"pdenv_policy_net_pg.ckpt\"\n",
    "\n",
    "with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        saver.restore(sess, model_file)\n",
    "        for episode in range(n_episodes):\n",
    "            state = env.reset()\n",
    "            for step in range(episode_length):\n",
    "                system.state = state\n",
    "                img = system.visualize()\n",
    "                frames.append(img)\n",
    "\n",
    "                action_val = action.eval(feed_dict={X: state.reshape(1, n_inputs)})\n",
    "                vect_action = int_to_action(action_val,env) #HERE WE CONVERT FROM INTEGER TO ACTION's Array\n",
    "                #print(vect_action)\n",
    "                state, reward, done, info = env.step(vect_action)\n",
    "                #print(state)\n",
    "                #print(action_val[0],emptiest_tank_policy(state, system))\n",
    "        env.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_anim = ut.create_system_animation(frames, n_episodes * episode_length)\n",
    "plt.close()\n",
    "\n",
    "HTML(test_anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
