{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dsalgador/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "The algorithm is tested on the PDSystemEnv  gym task \n",
    "and developed with Tensorflow\n",
    "\n",
    "Author: Daniel Salgado Rojo\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gym\n",
    "import gym_pdsystem\n",
    "from gym import wrappers\n",
    "#import tflearn\n",
    "import argparse\n",
    "import pprint as pp\n",
    "\n",
    "#from ddpg.replay_buffer import ReplayBuffer\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import random\n",
    "from gym_pdsystem.envs.pdsystem_env import PDSystemEnv\n",
    "\n",
    "import gym_pdsystem.utils.utilsq as ut\n",
    "import gym_pdsystem.utils.constants as ct\n",
    "\n",
    "\n",
    "from matplotlib import animation, rc\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML\n",
    "\n",
    "import time\n",
    "\n",
    "#TO OMMIT WARNINGS\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# Example n=5, k = 2\n",
    "\n",
    "TANK_MAX_LOADS = np.array([100., 200, 100., 800., 200.])\n",
    "LEVEL_PERCENTAGES = np.array([ #b , c, e\n",
    "                                                [0.02, 0.31, 0.9],\n",
    "                                                [0.01, 0.03, 0.9],\n",
    "                                                [0.05, 0.16, 0.9],\n",
    "                                                [0.07, 0.14, 0.85],\n",
    "                                                [0.08, 0.26, 0.9]\n",
    "                                                   ])\n",
    "\n",
    "TRUCK_MAX_LOADS = np.array([70.,130.])\n",
    "\n",
    "GRAPH_WEIGHTS = np.array([32., 159., 162., 156.,156., 0.])\n",
    "DISCRETE = True\n",
    "############################################################\n",
    "\n",
    "\n",
    "\n",
    "###########################################################\n",
    "# Example n=9, k = 3\n",
    "\n",
    "TANK_MAX_LOADS = np.array([100., 200, 100., 800., 200., 500., 300., 800., 300.])\n",
    "LEVEL_PERCENTAGES = np.array([ #b , c, e\n",
    "                                                [0.02, 0.31, 0.9],\n",
    "                                                [0.01, 0.03, 0.9],\n",
    "                                                [0.05, 0.16, 0.9],\n",
    "                                                [0.07, 0.14, 0.85],\n",
    "                                                [0.08, 0.26, 0.9],\n",
    "                                                [0.02, 0.31, 0.9],\n",
    "                                                [0.01, 0.03, 0.9],\n",
    "                                                [0.05, 0.16, 0.9],\n",
    "                                                [0.07, 0.14, 0.85]\n",
    "                                                   ])\n",
    "\n",
    "TRUCK_MAX_LOADS = np.array([70.,130.,250.])\n",
    "\n",
    "GRAPH_WEIGHTS = np.array([32., 159., 162., 156.,156., 32., 159., 162., 156., 0.])\n",
    "DISCRETE = True\n",
    "############################################################\n",
    "\n",
    "\n",
    "env = gym.make(\"PDSystemEnv-v0\")\n",
    "episode_length = 30\n",
    "env._max_episode_steps = episode_length\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def int_to_action(int_action: int, env):\n",
    "    \"\"\"\n",
    "    So far assumed k = 2 or 3:\n",
    "    \n",
    "    Converts an integer between 0 and env.action_space.shape[1]**env.action_space.shape[0]\n",
    "    which is (n+1)^k where n is the number of tanks and k the number of trucks.\n",
    "    \n",
    "    return vect_action: a k-dimensional vector with components in the range 0,...n. \n",
    "    For k = 2, vect_action = [i,j] is the action of truck 1 going to tank i and truck 2 going to tank j.\n",
    "    (i, j = n means staying at the depot, 0,....,n-1 are the real tanks).\n",
    "    The associated integer is i*(n+1) + j\n",
    "    \n",
    "    For k = 3, vect_action = [i,j,l] is the action of truck 1 going to tank i, truck 2 going to tank j,\n",
    "    and truck 3 going to tank l.\n",
    "    (i, j, l = n means staying at the depot, 0,....,n-1 are the real tanks).\n",
    "    The associated integer is (i*(n+1) + j)*(n+1) + l\n",
    "\n",
    "    \"\"\"\n",
    "    nplus1 = env.action_space.shape[1]\n",
    "    k = env.action_space.shape[0]\n",
    "    n_actions = nplus1**k\n",
    "    \n",
    "    if k == 2:\n",
    "        j = int_action % nplus1\n",
    "        i = int((int_action-j)/nplus1)\n",
    "        vect_action = np.array([i,j])\n",
    "      \n",
    "    elif k == 3:\n",
    "        l = int_action % nplus1\n",
    "        ij = int( (int_action - l)/nplus1 ) \n",
    "        j = ij % nplus1\n",
    "        i = int((ij-j)/nplus1)\n",
    "        vect_action = np.array([i,j,l])\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"The number of trucks k of the environment is different from 2 or 3\")\n",
    "    return vect_action\n",
    "\n",
    "def action_to_int(vect_action: np.array, env):\n",
    "    \"\"\"\n",
    "    Assumed k = 2 or 3, so vect_action has 2 or 3 components\n",
    "    \"\"\"\n",
    "    nplus1 = env.action_space.shape[1]\n",
    "    k = env.action_space.shape[0]\n",
    "    if k == 2:\n",
    "        int_action = vect_action[0] * nplus1 + vect_action[1]\n",
    "    elif k == 3:\n",
    "        int_action = (vect_action[0] * nplus1 + vect_action[1])*nplus1 + vect_action[2]\n",
    "    else:\n",
    "        raise ValueError(\"The number of trucks k of the environment is different from 2 or 3\")\n",
    "\n",
    "    return int_action\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66\n",
      "[0 6 6]\n",
      "66\n"
     ]
    }
   ],
   "source": [
    "int_action = 66\n",
    "print(int_action)\n",
    "vect_action = int_to_action(int_action,env)\n",
    "print(vect_action)\n",
    "int_action = action_to_int(vect_action,env)\n",
    "print(int_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/dsalgador/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "seed = 42\n",
    "learning_rate = 0.01 #0.01\n",
    "\n",
    "model_file = './final_pgmodel.ckpt'#.format(learning_rate)\n",
    "graph_file =  '{}.meta'.format(model_file)\n",
    "#epochs = 100 #2000\n",
    "#batch_size = 50 #50\n",
    "\n",
    "summary_freq = 100#np.ceil(epochs/10) #200\n",
    "\n",
    "\n",
    "hidden1_neurons = 300 #100\n",
    "hidden2_neurons = 300 #50\n",
    "\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "            #inputs = tflearn.input_data(shape=[None, self.s_dim])\n",
    "            tf.set_random_seed(seed)\n",
    "            # 1. Parameters to determine the NN architecture\n",
    "\n",
    "            n_inputs = env.observation_space.shape[1]\n",
    "            n_hidden1 = hidden1_neurons; activation1 = tf.nn.sigmoid#tf.nn.elu\n",
    "            n_hidden2 = hidden2_neurons; activation2 = tf.nn.sigmoid#tf.nn.elu\n",
    "            n_outputs = env.action_space.shape[1]**env.action_space.shape[0]\n",
    "            \n",
    "            #indices = [i for i in range(n_outputs)]\n",
    "            \n",
    "            initializer = tf.contrib.layers.variance_scaling_initializer()\n",
    "\n",
    "            # 2. Build the Neural Network\n",
    "            \n",
    "            X = tf.placeholder(tf.float32, shape = (None, n_inputs), name = \"X\")\n",
    "            #y = tf.placeholder(tf.int64, shape = (None), name = \"y\")\n",
    "            \n",
    "            hidden1 = tf.layers.dense(X, n_hidden1, activation = activation1,\n",
    "                                     kernel_initializer = initializer)\n",
    "            hidden2 = tf.layers.dense(hidden1, n_hidden2, activation = activation2,\n",
    "                                     kernel_initializer = initializer)\n",
    "            logits = tf.layers.dense(hidden2, n_outputs)#,kernel_initializer = initializer)\n",
    "            outputs = tf.nn.softmax(logits)\n",
    "                   \n",
    "            \n",
    "with tf.name_scope(\"action\"):\n",
    "            # 3. Select a random action (where to go) based on the estimated probabilities\n",
    "            action = tf.multinomial(tf.log(outputs), num_samples = 1)\n",
    "            #print(tf.rank(action))\n",
    "            #action_onehot = tf.one_hot(indices, n_outputs)\n",
    "            #y = tf.reshape(action_onehot[action], (n_outputs, None))\n",
    "            #y = tf.Variable(action, tf.int64)\n",
    "            y = tf.reshape(action, [1])\n",
    "            #print(tf.rank(y))\n",
    "            #print(tf.rank(logits))\n",
    "            \n",
    "with tf.name_scope(\"loss\"):\n",
    "            xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y,logits = logits)\n",
    "#             xentropy = tf.nn.sigmoid_cross_entropy_with_logits(labels = y,\n",
    "#                                                                       logits = logits)\n",
    "            loss = tf.reduce_mean(xentropy, name = \"loss\")\n",
    "            \n",
    "tf.summary.scalar('average_cross_entropy', loss)\n",
    "\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "            # Optimization Op\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "            #optimize = optimizer.minimize(loss)\n",
    "            \n",
    "            grads_and_vars = optimizer.compute_gradients(xentropy)\n",
    "            gradients = [grad for grad, variable in grads_and_vars]\n",
    "            gradient_placeholders = []\n",
    "            grads_and_vars_feed = []\n",
    "            for grad, variable in grads_and_vars:\n",
    "                gradient_placeholder = tf.placeholder(tf.float32, shape=grad.get_shape())\n",
    "                gradient_placeholders.append(gradient_placeholder)\n",
    "                grads_and_vars_feed.append((gradient_placeholder, variable))\n",
    "            training_op = optimizer.apply_gradients(grads_and_vars_feed)\n",
    "\n",
    "                        \n",
    "# # with tf.name_scope(\"eval\"):\n",
    "# #             correct = tf.nn.in_top_k(logits, y, 1)\n",
    "# #             accuracy = tf.reduce_mean(tf.cast(correct,tf.float32))\n",
    "# # tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "                       \n",
    "merged = tf.summary.merge_all()\n",
    "init = tf.global_variables_initializer()\n",
    "        \n",
    "if model_file != None:\n",
    "            saver = tf.train.Saver()\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "From https://github.com/ageron/handson-ml \n",
    "\"\"\"\n",
    "\n",
    "def discount_rewards(rewards, discount_rate):\n",
    "    discounted_rewards = np.zeros(len(rewards))\n",
    "    cumulative_rewards = 0\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        cumulative_rewards = rewards[step] + cumulative_rewards * discount_rate\n",
    "        discounted_rewards[step] = cumulative_rewards\n",
    "    return discounted_rewards\n",
    "\n",
    "def discount_and_normalize_rewards(all_rewards, discount_rate):\n",
    "    all_discounted_rewards = [discount_rewards(rewards, discount_rate) for rewards in all_rewards]\n",
    "    flat_rewards = np.concatenate(all_discounted_rewards)\n",
    "    reward_mean = flat_rewards.mean()\n",
    "    reward_std = flat_rewards.std()\n",
    "    return [(discounted_rewards - reward_mean)/reward_std for discounted_rewards in all_discounted_rewards]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0. Average reward per game:  -0.46773 , Elapsed time  0.01  minutes.\n",
      "Iteration: 100. Average reward per game:  -0.285488 , Elapsed time  1.46  minutes.\n",
      "Iteration: 200. Average reward per game:  -0.274483 , Elapsed time  2.95  minutes.\n",
      "Iteration: 300. Average reward per game:  -0.315922 , Elapsed time  4.41  minutes.\n",
      "Iteration: 400. Average reward per game:  -0.449572 , Elapsed time  5.88  minutes.\n",
      "Iteration: 500. Average reward per game:  -0.306776 , Elapsed time  7.35  minutes.\n",
      "Iteration: 600. Average reward per game:  -0.362806 , Elapsed time  8.86  minutes.\n",
      "Iteration: 700. Average reward per game:  -0.299006 , Elapsed time  10.29  minutes.\n",
      "Iteration: 800. Average reward per game:  -0.299134 , Elapsed time  11.77  minutes.\n",
      "Iteration: 900. Average reward per game:  -0.318637 , Elapsed time  13.24  minutes.\n",
      "Iteration: 1000. Average reward per game:  -0.345986 , Elapsed time  14.72  minutes.\n",
      "Iteration: 1100. Average reward per game:  -0.451839 , Elapsed time  16.27  minutes.\n",
      "Iteration: 1200. Average reward per game:  -0.417295 , Elapsed time  17.77  minutes.\n",
      "Iteration: 1300. Average reward per game:  -0.284415 , Elapsed time  19.26  minutes.\n",
      "Iteration: 1400. Average reward per game:  -0.456498 , Elapsed time  20.75  minutes.\n",
      "Iteration: 1500. Average reward per game:  -0.289348 , Elapsed time  22.32  minutes.\n",
      "Iteration: 1600. Average reward per game:  -0.342941 , Elapsed time  23.85  minutes.\n",
      "Iteration: 1700. Average reward per game:  -0.345575 , Elapsed time  25.4  minutes.\n",
      "Iteration: 1800. Average reward per game:  -0.28973 , Elapsed time  26.94  minutes.\n",
      "Iteration: 1900. Average reward per game:  -0.284875 , Elapsed time  28.46  minutes.\n",
      "Iteration: 2000. Average reward per game:  -0.301164 , Elapsed time  29.93  minutes.\n",
      "Iteration: 2100. Average reward per game:  -0.313411 , Elapsed time  31.46  minutes.\n",
      "Iteration: 2200. Average reward per game:  -0.298094 , Elapsed time  32.97  minutes.\n",
      "Iteration: 2300. Average reward per game:  -0.28492 , Elapsed time  34.42  minutes.\n",
      "Iteration: 2400. Average reward per game:  -0.303486 , Elapsed time  35.98  minutes.\n",
      "Iteration: 2500. Average reward per game:  -0.493317 , Elapsed time  37.46  minutes.\n",
      "Iteration: 2600. Average reward per game:  -0.392137 , Elapsed time  38.94  minutes.\n",
      "Iteration: 2700. Average reward per game:  -0.297792 , Elapsed time  40.49  minutes.\n",
      "Iteration: 2800. Average reward per game:  -0.314576 , Elapsed time  42.09  minutes.\n",
      "Iteration: 2900. Average reward per game:  -0.292473 , Elapsed time  43.62  minutes.\n",
      "Iteration: 3000. Average reward per game:  -0.30314 , Elapsed time  45.15  minutes.\n",
      "Iteration: 3100. Average reward per game:  -0.321225 , Elapsed time  46.74  minutes.\n",
      "Iteration: 3200. Average reward per game:  -0.4223 , Elapsed time  48.32  minutes.\n",
      "Iteration: 3300. Average reward per game:  -0.347322 , Elapsed time  49.88  minutes.\n",
      "Iteration: 3400. Average reward per game:  -0.301462 , Elapsed time  51.41  minutes.\n",
      "Iteration: 3500. Average reward per game:  -0.289168 , Elapsed time  52.98  minutes.\n",
      "Iteration: 3600. Average reward per game:  -0.285269 , Elapsed time  54.58  minutes.\n",
      "Iteration: 3700. Average reward per game:  -0.38238 , Elapsed time  56.16  minutes.\n",
      "Iteration: 3800. Average reward per game:  -0.304178 , Elapsed time  57.65  minutes.\n",
      "Iteration: 3900. Average reward per game:  -0.411415 , Elapsed time  59.23  minutes.\n",
      "Iteration: 4000. Average reward per game:  -0.307832 , Elapsed time  60.75  minutes.\n",
      "Iteration: 4100. Average reward per game:  -0.330544 , Elapsed time  62.33  minutes.\n",
      "Iteration: 4200. Average reward per game:  -0.296511 , Elapsed time  63.88  minutes.\n",
      "Iteration: 4300. Average reward per game:  -0.400207 , Elapsed time  65.43  minutes.\n",
      "Iteration: 4400. Average reward per game:  -0.296434 , Elapsed time  67.06  minutes.\n",
      "Iteration: 4500. Average reward per game:  -0.295158 , Elapsed time  68.65  minutes.\n",
      "Iteration: 4600. Average reward per game:  -0.30664 , Elapsed time  70.21  minutes.\n",
      "Iteration: 4700. Average reward per game:  -0.31268 , Elapsed time  72.04  minutes.\n",
      "Iteration: 4800. Average reward per game:  -0.30681 , Elapsed time  73.91  minutes.\n",
      "Iteration: 4900. Average reward per game:  -0.304332 , Elapsed time  75.58  minutes.\n",
      "Iteration: 5000. Average reward per game:  -0.303785 , Elapsed time  77.2  minutes.\n",
      "Iteration: 5100. Average reward per game:  -0.299158 , Elapsed time  78.82  minutes.\n",
      "Iteration: 5200. Average reward per game:  -0.310078 , Elapsed time  80.33  minutes.\n",
      "Iteration: 5300. Average reward per game:  -0.312509 , Elapsed time  81.88  minutes.\n",
      "Iteration: 5400. Average reward per game:  -0.416094 , Elapsed time  83.42  minutes.\n",
      "Iteration: 5500. Average reward per game:  -0.315527 , Elapsed time  84.99  minutes.\n",
      "Iteration: 5600. Average reward per game:  -0.325034 , Elapsed time  86.61  minutes.\n",
      "Iteration: 5700. Average reward per game:  -0.285244 , Elapsed time  88.1  minutes.\n",
      "Iteration: 5800. Average reward per game:  -0.292847 , Elapsed time  89.89  minutes.\n",
      "Iteration: 5900. Average reward per game:  -0.288483 , Elapsed time  91.5  minutes.\n",
      "Iteration: 6000. Average reward per game:  -0.298338 , Elapsed time  93.14  minutes.\n",
      "Iteration: 6100. Average reward per game:  -0.354505 , Elapsed time  94.73  minutes.\n",
      "Iteration: 6200. Average reward per game:  -0.304269 , Elapsed time  96.35  minutes.\n",
      "Iteration: 6300. Average reward per game:  -0.297001 , Elapsed time  98.0  minutes.\n",
      "Iteration: 6400. Average reward per game:  -0.297196 , Elapsed time  99.59  minutes.\n",
      "Iteration: 6500. Average reward per game:  -0.332522 , Elapsed time  101.13  minutes.\n",
      "Iteration: 6600. Average reward per game:  -0.315825 , Elapsed time  102.8  minutes.\n",
      "Iteration: 6700. Average reward per game:  -0.29568 , Elapsed time  104.48  minutes.\n",
      "Iteration: 6800. Average reward per game:  -0.292495 , Elapsed time  106.13  minutes.\n",
      "Iteration: 6900. Average reward per game:  -0.316864 , Elapsed time  107.73  minutes.\n",
      "Iteration: 7000. Average reward per game:  -0.29628 , Elapsed time  109.4  minutes.\n",
      "Iteration: 7100. Average reward per game:  -0.331576 , Elapsed time  110.99  minutes.\n",
      "Iteration: 7200. Average reward per game:  -0.302639 , Elapsed time  112.58  minutes.\n",
      "Iteration: 7300. Average reward per game:  -0.29849 , Elapsed time  114.11  minutes.\n",
      "Iteration: 7400. Average reward per game:  -0.288559 , Elapsed time  115.67  minutes.\n",
      "Iteration: 7500. Average reward per game:  -0.288586 , Elapsed time  117.32  minutes.\n",
      "Iteration: 7600. Average reward per game:  -0.270493 , Elapsed time  119.09  minutes.\n",
      "Iteration: 7700. Average reward per game:  -0.320951 , Elapsed time  120.77  minutes.\n",
      "Iteration: 7800. Average reward per game:  -0.300235 , Elapsed time  122.47  minutes.\n",
      "Iteration: 7900. Average reward per game:  -0.303545 , Elapsed time  124.19  minutes.\n",
      "Iteration: 8000. Average reward per game:  -0.333217 , Elapsed time  125.81  minutes.\n",
      "Iteration: 8100. Average reward per game:  -0.305189 , Elapsed time  127.41  minutes.\n",
      "Iteration: 8200. Average reward per game:  -0.271731 , Elapsed time  129.07  minutes.\n",
      "Iteration: 8300. Average reward per game:  -0.269214 , Elapsed time  130.67  minutes.\n",
      "Iteration: 8400. Average reward per game:  -0.295865 , Elapsed time  132.29  minutes.\n",
      "Iteration: 8500. Average reward per game:  -0.305332 , Elapsed time  133.9  minutes.\n",
      "Iteration: 8600. Average reward per game:  -0.311735 , Elapsed time  135.48  minutes.\n",
      "Iteration: 8700. Average reward per game:  -0.314542 , Elapsed time  137.15  minutes.\n",
      "Iteration: 8800. Average reward per game:  -0.313798 , Elapsed time  138.79  minutes.\n",
      "Iteration: 8900. Average reward per game:  -0.310441 , Elapsed time  140.39  minutes.\n",
      "Iteration: 9000. Average reward per game:  -0.283794 , Elapsed time  142.1  minutes.\n",
      "Iteration: 9100. Average reward per game:  -0.30999 , Elapsed time  143.79  minutes.\n",
      "Iteration: 9200. Average reward per game:  -0.347928 , Elapsed time  145.41  minutes.\n",
      "Iteration: 9300. Average reward per game:  -0.357817 , Elapsed time  147.04  minutes.\n",
      "Iteration: 9400. Average reward per game:  -0.302273 , Elapsed time  148.63  minutes.\n",
      "Iteration: 9500. Average reward per game:  -0.294628 , Elapsed time  150.4  minutes.\n",
      "Iteration: 9600. Average reward per game:  -0.403935 , Elapsed time  152.02  minutes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 9700. Average reward per game:  -0.276245 , Elapsed time  153.64  minutes.\n",
      "Iteration: 9800. Average reward per game:  -0.331254 , Elapsed time  155.33  minutes.\n",
      "Iteration: 9900. Average reward per game:  -0.327473 , Elapsed time  157.01  minutes.\n",
      "Iteration: 10000. Average reward per game:  -0.344761 , Elapsed time  158.71  minutes.\n",
      "Iteration: 10100. Average reward per game:  -0.320952 , Elapsed time  160.36  minutes.\n",
      "Iteration: 10200. Average reward per game:  -0.300262 , Elapsed time  162.07  minutes.\n",
      "Iteration: 10300. Average reward per game:  -0.316352 , Elapsed time  163.69  minutes.\n",
      "Iteration: 10400. Average reward per game:  -0.283391 , Elapsed time  165.35  minutes.\n",
      "Iteration: 10500. Average reward per game:  -0.332011 , Elapsed time  167.07  minutes.\n",
      "Iteration: 10600. Average reward per game:  -0.301473 , Elapsed time  168.71  minutes.\n",
      "Iteration: 10700. Average reward per game:  -0.311141 , Elapsed time  170.38  minutes.\n",
      "Iteration: 10800. Average reward per game:  -0.301804 , Elapsed time  172.07  minutes.\n",
      "Iteration: 10900. Average reward per game:  -0.344776 , Elapsed time  173.82  minutes.\n",
      "Iteration: 11000. Average reward per game:  -0.303155 , Elapsed time  175.49  minutes.\n",
      "Iteration: 11100. Average reward per game:  -0.310219 , Elapsed time  177.22  minutes.\n",
      "Iteration: 11192"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-8754581f0ad2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mvar_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient_placeholder\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient_placeholders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             mean_gradients = np.mean([reward * all_gradients[game_index][step][var_index]\n\u001b[0;32m---> 61\u001b[0;31m                                       \u001b[0;32mfor\u001b[0m \u001b[0mgame_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_rewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m                                           for step, reward in enumerate(rewards)], axis=0)\n\u001b[1;32m     63\u001b[0m             \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgradient_placeholder\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean_gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-8754581f0ad2>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     60\u001b[0m             mean_gradients = np.mean([reward * all_gradients[game_index][step][var_index]\n\u001b[1;32m     61\u001b[0m                                       \u001b[0;32mfor\u001b[0m \u001b[0mgame_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_rewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m                                           for step, reward in enumerate(rewards)], axis=0)\n\u001b[0m\u001b[1;32m     63\u001b[0m             \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgradient_placeholder\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean_gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "\n",
    "best_acc_val = 0\n",
    "acc_val = 0\n",
    "\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir, now)   \n",
    "\n",
    "n_games_per_update = 10\n",
    "n_max_steps = episode_length\n",
    "n_iterations = 10**5\n",
    "save_iterations = 100\n",
    "discount_rate = 0.95\n",
    "\n",
    "summary_freq = 100\n",
    "round_time = 2\n",
    "round_reward = 6\n",
    "avg_rewards_list = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    #train_writer = tf.summary.FileWriter(logdir + '/pgtrain', sess.graph)\n",
    "\n",
    "    init.run()\n",
    "    time_start = time.time()\n",
    "\n",
    "    for iteration in range(n_iterations+1):\n",
    "        print(\"\\rIteration: {}\".format(iteration), end=\"\")\n",
    "        all_rewards = []\n",
    "        all_gradients = []\n",
    "        for game in range(n_games_per_update):\n",
    "            current_rewards = []\n",
    "            current_gradients = []\n",
    "            obs = env.reset()\n",
    "            for step in range(n_max_steps):\n",
    "                action_val, gradients_val = sess.run([action, gradients], feed_dict={X: obs.reshape(1, n_inputs)}) \n",
    "                vect_action = int_to_action(action_val,env) #HERE WE CONVERT FROM INTEGER TO ACTION's Array\n",
    "                obs, reward, done, info = env.step(vect_action)\n",
    "                current_rewards.append(reward)\n",
    "                current_gradients.append(gradients_val)\n",
    "                \n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "            all_rewards.append(current_rewards)\n",
    "            all_gradients.append(current_gradients)\n",
    "            \n",
    "        if iteration % summary_freq == 0:  \n",
    "            time_end = time.time()\n",
    "            avg_rewards = np.mean(np.array(all_rewards))\n",
    "            avg_rewards_list.append(avg_rewards)\n",
    "            \n",
    "            print(\". Average reward per game: \",  round(avg_rewards / n_games_per_update, round_reward), \n",
    "                  \", Elapsed time \", round( (time_end-time_start)/60., round_time), \" minutes.\")    \n",
    "\n",
    "        all_rewards = discount_and_normalize_rewards(all_rewards, discount_rate=discount_rate)\n",
    "        feed_dict = {}\n",
    "        for var_index, gradient_placeholder in enumerate(gradient_placeholders):\n",
    "            mean_gradients = np.mean([reward * all_gradients[game_index][step][var_index]\n",
    "                                      for game_index, rewards in enumerate(all_rewards)\n",
    "                                          for step, reward in enumerate(rewards)], axis=0)\n",
    "            feed_dict[gradient_placeholder] = mean_gradients\n",
    "         \n",
    "       \n",
    "        #tf.summary.scalar('avg_rewards', avg_rewards)\n",
    "\n",
    "\n",
    "        #train_writer.add_summary(avg_rewards, iteration)\n",
    "\n",
    "\n",
    "        sess.run(training_op, feed_dict=feed_dict)\n",
    "        if iteration % save_iterations == 0:\n",
    "                  saver.save(sess, \"./pdenv_policy_net_pg.ckpt\")\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Iteration: 0Average reward per game:  -131.21281576006612\n",
    "# Iteration: 100Average reward per game:  -92.23713538883702\n",
    "# Iteration: 200Average reward per game:  -70.92879872240424\n",
    "# Iteration: 300Average reward per game:  -46.36746628903998\n",
    "# Iteration: 400Average reward per game:  -43.44814607253152\n",
    "# Iteration: 500Average reward per game:  -26.28340975064497\n",
    "# Iteration: 600Average reward per game:  -24.65947230415822\n",
    "# Iteration: 700Average reward per game:  -30.107651383652463\n",
    "# Iteration: 800Average reward per game:  -17.534408167806813\n",
    "# Iteration: 900Average reward per game:  -19.252239544660405\n",
    "# Iteration: 1000Average reward per game:  -20.431260399119516"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PLAY:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seed = 45\n",
    "np.random.seed(seed)\n",
    "\n",
    "frames = []\n",
    "n_episodes = 5\n",
    "\n",
    "system = PDSystemEnv()\n",
    "\n",
    "model_file = \"pdenv_policy_net_pg.ckpt\"\n",
    "\n",
    "with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        saver.restore(sess, model_file)\n",
    "        for episode in range(n_episodes):\n",
    "            state = env.reset()\n",
    "            for step in range(episode_length):\n",
    "                system.state = state\n",
    "                img = system.visualize()\n",
    "                frames.append(img)\n",
    "\n",
    "                action_val = action.eval(feed_dict={X: state.reshape(1, n_inputs)})\n",
    "                vect_action = int_to_action(action_val,env) #HERE WE CONVERT FROM INTEGER TO ACTION's Array\n",
    "                #print(vect_action)\n",
    "                state, reward, done, info = env.step(vect_action)\n",
    "                #print(state)\n",
    "                #print(action_val[0],emptiest_tank_policy(state, system))\n",
    "        env.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_anim = ut.create_system_animation(frames, n_episodes * episode_length)\n",
    "plt.close()\n",
    "\n",
    "HTML(test_anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.close()\n",
    "\n",
    "#THE REAL LEVELS (percentages 12h, 36h, ? h)\n",
    "tank_levels = [frames[i][2] for i in range(len(frames))]\n",
    "tank_levels_array = np.asarray(tank_levels).transpose()\n",
    "\n",
    "n = system.n\n",
    "\n",
    "cmap = plt.get_cmap('gnuplot')\n",
    "colors = [cmap(i) for i in np.linspace(0, 1, n)]\n",
    "lvl_colors = [\"Orange\", \"Green\",\"Orange\"]\n",
    "\n",
    "tanks_max_load = system.tank_max_loads\n",
    "level_percentages = system.load_level_percentages\n",
    "\n",
    "for i, color in enumerate(colors, start=1):\n",
    "    plt.subplot(3,3, i)    \n",
    "\n",
    "    plt.plot(tank_levels_array[i-1], color=color, label='Tank ${i}$'.format(i=i))\n",
    "    \n",
    "    plt.axhline(y= tanks_max_load[i-1], xmin=0, xmax=episode_length, hold=None, color = \"Red\", linestyle = '--')\n",
    "    for lvl_color, lvl in zip(lvl_colors, level_percentages[i-1]):\n",
    "        plt.axhline(y= lvl * tanks_max_load[i-1], xmin=0, xmax=episode_length, hold=None, color = lvl_color, \n",
    "                    linestyle = '--')\n",
    "    plt.axhline(y= 0, xmin=0, xmax=episode_length, hold=None, color = \"Red\", linestyle = '--')\n",
    "    \n",
    "    \n",
    "    percentages = level_percentages[i-1]           \n",
    "    c = percentages[1]\n",
    "    e = percentages[2]          \n",
    "    d = ct.p0_GLOBAL*e+(1-ct.p0_GLOBAL)*c\n",
    "    plt.axhline(y= d*tanks_max_load[i-1], xmin=0, xmax=episode_length, hold=None, color = \"lawngreen\", \n",
    "                linestyle = '-.')\n",
    "\n",
    "    plt.axhline(y= np.mean(tank_levels_array[i-1]), xmin=0, xmax=episode_length, hold=None, \n",
    "                color = \"blue\", linestyle = '-.')\n",
    "    \n",
    "    plt.legend(loc='best')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "int_to_action(352,env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
