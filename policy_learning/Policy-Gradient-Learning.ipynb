{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dsalgador/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "The algorithm is tested on the PDSystemEnv  gym task \n",
    "and developed with Tensorflow\n",
    "\n",
    "Author: Daniel Salgado Rojo\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gym\n",
    "import gym_pdsystem\n",
    "from gym import wrappers\n",
    "#import tflearn\n",
    "import argparse\n",
    "import pprint as pp\n",
    "\n",
    "#from ddpg.replay_buffer import ReplayBuffer\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import random\n",
    "from gym_pdsystem.envs.pdsystem_env import PDSystemEnv\n",
    "\n",
    "import gym_pdsystem.utils.utilsq as ut\n",
    "import gym_pdsystem.utils.constants as ct\n",
    "\n",
    "\n",
    "from matplotlib import animation, rc\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML\n",
    "\n",
    "\n",
    "#TO OMMIT WARNINGS\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# Example n=5, k = 2\n",
    "\n",
    "TANK_MAX_LOADS = np.array([100., 200, 100., 800., 200.])\n",
    "LEVEL_PERCENTAGES = np.array([ #b , c, e\n",
    "                                                [0.02, 0.31, 0.9],\n",
    "                                                [0.01, 0.03, 0.9],\n",
    "                                                [0.05, 0.16, 0.9],\n",
    "                                                [0.07, 0.14, 0.85],\n",
    "                                                [0.08, 0.26, 0.9]\n",
    "                                                   ])\n",
    "\n",
    "TRUCK_MAX_LOADS = np.array([70.,130.])\n",
    "\n",
    "GRAPH_WEIGHTS = np.array([32., 159., 162., 156.,156., 0.])\n",
    "DISCRETE = True\n",
    "############################################################\n",
    "\n",
    "\n",
    "\n",
    "###########################################################\n",
    "# Example n=9, k = 3\n",
    "\n",
    "TANK_MAX_LOADS = np.array([100., 200, 100., 800., 200., 500., 300., 800., 300.])\n",
    "LEVEL_PERCENTAGES = np.array([ #b , c, e\n",
    "                                                [0.02, 0.31, 0.9],\n",
    "                                                [0.01, 0.03, 0.9],\n",
    "                                                [0.05, 0.16, 0.9],\n",
    "                                                [0.07, 0.14, 0.85],\n",
    "                                                [0.08, 0.26, 0.9],\n",
    "                                                [0.02, 0.31, 0.9],\n",
    "                                                [0.01, 0.03, 0.9],\n",
    "                                                [0.05, 0.16, 0.9],\n",
    "                                                [0.07, 0.14, 0.85]\n",
    "                                                   ])\n",
    "\n",
    "TRUCK_MAX_LOADS = np.array([70.,130.,250.])\n",
    "\n",
    "GRAPH_WEIGHTS = np.array([32., 159., 162., 156.,156., 32., 159., 162., 156., 0.])\n",
    "DISCRETE = True\n",
    "############################################################\n",
    "\n",
    "\n",
    "env = gym.make(\"PDSystemEnv-v0\")\n",
    "episode_length = 30\n",
    "env._max_episode_steps = episode_length\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def int_to_action(int_action: int, env):\n",
    "    \"\"\"\n",
    "    So far assumed k = 2 or 3:\n",
    "    \n",
    "    Converts an integer between 0 and env.action_space.shape[1]**env.action_space.shape[0]\n",
    "    which is (n+1)^k where n is the number of tanks and k the number of trucks.\n",
    "    \n",
    "    return vect_action: a k-dimensional vector with components in the range 0,...n. \n",
    "    For k = 2, vect_action = [i,j] is the action of truck 1 going to tank i and truck 2 going to tank j.\n",
    "    (i, j = n means staying at the depot, 0,....,n-1 are the real tanks).\n",
    "    The associated integer is i*(n+1) + j\n",
    "    \n",
    "    For k = 3, vect_action = [i,j,l] is the action of truck 1 going to tank i, truck 2 going to tank j,\n",
    "    and truck 3 going to tank l.\n",
    "    (i, j, l = n means staying at the depot, 0,....,n-1 are the real tanks).\n",
    "    The associated integer is (i*(n+1) + j)*(n+1) + l\n",
    "\n",
    "    \"\"\"\n",
    "    nplus1 = env.action_space.shape[1]\n",
    "    k = env.action_space.shape[0]\n",
    "    n_actions = nplus1**k\n",
    "    \n",
    "    if k == 2:\n",
    "        j = int_action % nplus1\n",
    "        i = int((int_action-j)/nplus1)\n",
    "        vect_action = np.array([i,j])\n",
    "      \n",
    "    elif k == 3:\n",
    "        l = int_action % nplus1\n",
    "        ij = int( (int_action - l)/nplus1 ) \n",
    "        j = ij % nplus1\n",
    "        i = int((ij-j)/nplus1)\n",
    "        vect_action = np.array([i,j,l])\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"The number of trucks k of the environment is different from 2 or 3\")\n",
    "    return vect_action\n",
    "\n",
    "def action_to_int(vect_action: np.array, env):\n",
    "    \"\"\"\n",
    "    Assumed k = 2 or 3, so vect_action has 2 or 3 components\n",
    "    \"\"\"\n",
    "    nplus1 = env.action_space.shape[1]\n",
    "    k = env.action_space.shape[0]\n",
    "    if k == 2:\n",
    "        int_action = vect_action[0] * nplus1 + vect_action[1]\n",
    "    elif k == 3:\n",
    "        int_action = (vect_action[0] * nplus1 + vect_action[1])*nplus1 + vect_action[2]\n",
    "    else:\n",
    "        raise ValueError(\"The number of trucks k of the environment is different from 2 or 3\")\n",
    "\n",
    "    return int_action\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66\n",
      "[0 6 6]\n",
      "66\n"
     ]
    }
   ],
   "source": [
    "int_action = 66\n",
    "print(int_action)\n",
    "vect_action = int_to_action(int_action,env)\n",
    "print(vect_action)\n",
    "int_action = action_to_int(vect_action,env)\n",
    "print(int_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/dsalgador/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "seed = 42\n",
    "learning_rate = 0.01 #0.01\n",
    "\n",
    "model_file = './final_pgmodel.ckpt'#.format(learning_rate)\n",
    "graph_file =  '{}.meta'.format(model_file)\n",
    "epochs = 100 #2000\n",
    "batch_size = 50 #50\n",
    "\n",
    "summary_freq = 100#np.ceil(epochs/10) #200\n",
    "\n",
    "\n",
    "hidden1_neurons = 200 #100\n",
    "hidden2_neurons = 100 #50\n",
    "\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "            #inputs = tflearn.input_data(shape=[None, self.s_dim])\n",
    "            tf.set_random_seed(seed)\n",
    "            # 1. Parameters to determine the NN architecture\n",
    "\n",
    "            n_inputs = env.observation_space.shape[1]\n",
    "            n_hidden1 = hidden1_neurons; activation1 = tf.nn.sigmoid#tf.nn.elu\n",
    "            n_hidden2 = hidden2_neurons; activation2 = tf.nn.sigmoid#tf.nn.elu\n",
    "            n_outputs = env.action_space.shape[1]**env.action_space.shape[0]\n",
    "            \n",
    "            #indices = [i for i in range(n_outputs)]\n",
    "            \n",
    "            initializer = tf.contrib.layers.variance_scaling_initializer()\n",
    "\n",
    "            # 2. Build the Neural Network\n",
    "            \n",
    "            X = tf.placeholder(tf.float32, shape = (None, n_inputs), name = \"X\")\n",
    "            #y = tf.placeholder(tf.int64, shape = (None), name = \"y\")\n",
    "            \n",
    "            hidden1 = tf.layers.dense(X, n_hidden1, activation = activation1,\n",
    "                                     kernel_initializer = initializer)\n",
    "            hidden2 = tf.layers.dense(hidden1, n_hidden2, activation = activation2,\n",
    "                                     kernel_initializer = initializer)\n",
    "            logits = tf.layers.dense(hidden2, n_outputs)#,kernel_initializer = initializer)\n",
    "            outputs = tf.nn.softmax(logits)\n",
    "                   \n",
    "            \n",
    "with tf.name_scope(\"action\"):\n",
    "            # 3. Select a random action (where to go) based on the estimated probabilities\n",
    "            action = tf.multinomial(tf.log(outputs), num_samples = 1)\n",
    "            #print(tf.rank(action))\n",
    "            #action_onehot = tf.one_hot(indices, n_outputs)\n",
    "            #y = tf.reshape(action_onehot[action], (n_outputs, None))\n",
    "            #y = tf.Variable(action, tf.int64)\n",
    "            y = tf.reshape(action, [1])\n",
    "            #print(tf.rank(y))\n",
    "            #print(tf.rank(logits))\n",
    "            \n",
    "with tf.name_scope(\"loss\"):\n",
    "            xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y,logits = logits)\n",
    "#             xentropy = tf.nn.sigmoid_cross_entropy_with_logits(labels = y,\n",
    "#                                                                       logits = logits)\n",
    "            loss = tf.reduce_mean(xentropy, name = \"loss\")\n",
    "            \n",
    "tf.summary.scalar('average_cross_entropy', loss)\n",
    "\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "            # Optimization Op\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "            #optimize = optimizer.minimize(loss)\n",
    "            \n",
    "            grads_and_vars = optimizer.compute_gradients(xentropy)\n",
    "            gradients = [grad for grad, variable in grads_and_vars]\n",
    "            gradient_placeholders = []\n",
    "            grads_and_vars_feed = []\n",
    "            for grad, variable in grads_and_vars:\n",
    "                gradient_placeholder = tf.placeholder(tf.float32, shape=grad.get_shape())\n",
    "                gradient_placeholders.append(gradient_placeholder)\n",
    "                grads_and_vars_feed.append((gradient_placeholder, variable))\n",
    "            training_op = optimizer.apply_gradients(grads_and_vars_feed)\n",
    "\n",
    "                        \n",
    "# # with tf.name_scope(\"eval\"):\n",
    "# #             correct = tf.nn.in_top_k(logits, y, 1)\n",
    "# #             accuracy = tf.reduce_mean(tf.cast(correct,tf.float32))\n",
    "# # tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "                       \n",
    "merged = tf.summary.merge_all()\n",
    "init = tf.global_variables_initializer()\n",
    "        \n",
    "if model_file != None:\n",
    "            saver = tf.train.Saver()\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "From https://github.com/ageron/handson-ml \n",
    "\"\"\"\n",
    "\n",
    "def discount_rewards(rewards, discount_rate):\n",
    "    discounted_rewards = np.zeros(len(rewards))\n",
    "    cumulative_rewards = 0\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        cumulative_rewards = rewards[step] + cumulative_rewards * discount_rate\n",
    "        discounted_rewards[step] = cumulative_rewards\n",
    "    return discounted_rewards\n",
    "\n",
    "def discount_and_normalize_rewards(all_rewards, discount_rate):\n",
    "    all_discounted_rewards = [discount_rewards(rewards, discount_rate) for rewards in all_rewards]\n",
    "    flat_rewards = np.concatenate(all_discounted_rewards)\n",
    "    reward_mean = flat_rewards.mean()\n",
    "    reward_std = flat_rewards.std()\n",
    "    return [(discounted_rewards - reward_mean)/reward_std for discounted_rewards in all_discounted_rewards]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0Average reward per game:  -0.45679236403908813\n",
      "Iteration: 100Average reward per game:  -0.3377100544935953\n",
      "Iteration: 200Average reward per game:  -0.23507009331495526\n",
      "Iteration: 300Average reward per game:  -0.17474357795472323\n",
      "Iteration: 400Average reward per game:  -0.20523497306874705\n",
      "Iteration: 500Average reward per game:  -0.1606388120780834\n",
      "Iteration: 600Average reward per game:  -0.20155155770444472\n",
      "Iteration: 700Average reward per game:  -0.15395661125038737\n",
      "Iteration: 800Average reward per game:  -0.17052063708453322\n",
      "Iteration: 900Average reward per game:  -0.1688711990517431\n",
      "Iteration: 1000Average reward per game:  -0.15355108703890652\n",
      "Iteration: 1100Average reward per game:  -0.14905048035674534\n",
      "Iteration: 1200Average reward per game:  -0.21839909625875023\n",
      "Iteration: 1300Average reward per game:  -0.17592112731088208\n",
      "Iteration: 1400Average reward per game:  -0.14361685219804174\n",
      "Iteration: 1500Average reward per game:  -0.14782791810857424\n",
      "Iteration: 1600Average reward per game:  -0.17663095411349886\n",
      "Iteration: 1700Average reward per game:  -0.15445937973105134\n",
      "Iteration: 1800Average reward per game:  -0.14898384727264452\n",
      "Iteration: 1900Average reward per game:  -0.16869211351393648\n",
      "Iteration: 2000Average reward per game:  -0.19693054577342597\n",
      "Iteration: 2100Average reward per game:  -0.13516245756188505\n",
      "Iteration: 2200Average reward per game:  -0.16840470326163953\n",
      "Iteration: 2300Average reward per game:  -0.18504146614982458\n",
      "Iteration: 2400Average reward per game:  -0.1507481928411719\n",
      "Iteration: 2500Average reward per game:  -0.14637726548661142\n",
      "Iteration: 2600Average reward per game:  -0.1489354094050986\n",
      "Iteration: 2700Average reward per game:  -0.14369195155265657\n",
      "Iteration: 2800Average reward per game:  -0.171216661827742\n",
      "Iteration: 2900Average reward per game:  -0.13707000972192498\n",
      "Iteration: 3000Average reward per game:  -0.13936970115799469\n",
      "Iteration: 3100Average reward per game:  -0.14771662556620144\n",
      "Iteration: 3200Average reward per game:  -0.15387126978151336\n",
      "Iteration: 3300Average reward per game:  -0.15258757518621963\n",
      "Iteration: 3400Average reward per game:  -0.13867893929574776\n",
      "Iteration: 3500Average reward per game:  -0.14940752836144838\n",
      "Iteration: 3600Average reward per game:  -0.1368443341489172\n",
      "Iteration: 3700Average reward per game:  -0.1490624517121539\n",
      "Iteration: 3800Average reward per game:  -0.1639640950174253\n",
      "Iteration: 3900Average reward per game:  -0.14117289887202272\n",
      "Iteration: 4000Average reward per game:  -0.16207077253850605\n",
      "Iteration: 4100Average reward per game:  -0.1354472736966632\n",
      "Iteration: 4200Average reward per game:  -0.14369662149622503\n",
      "Iteration: 4300Average reward per game:  -0.13131277627739996\n",
      "Iteration: 4400Average reward per game:  -0.12051847087909025\n",
      "Iteration: 4500Average reward per game:  -0.14549570727440136\n",
      "Iteration: 4600Average reward per game:  -0.13999669990875258\n",
      "Iteration: 4700Average reward per game:  -0.13339741972810898\n",
      "Iteration: 4800Average reward per game:  -0.1352753219720537\n",
      "Iteration: 4900Average reward per game:  -0.14501685083069973\n",
      "Iteration: 5000Average reward per game:  -0.11607885350479916\n",
      "Iteration: 5100Average reward per game:  -0.11973737838904541\n",
      "Iteration: 5200Average reward per game:  -0.13705609312547212\n",
      "Iteration: 5300Average reward per game:  -0.1587741069598607\n",
      "Iteration: 5400Average reward per game:  -0.12650066508228042\n",
      "Iteration: 5500Average reward per game:  -0.13478762439228895\n",
      "Iteration: 5600Average reward per game:  -0.1634275919288745\n",
      "Iteration: 5700Average reward per game:  -0.13834723215262468\n",
      "Iteration: 5800Average reward per game:  -0.15060428653059116\n",
      "Iteration: 5900Average reward per game:  -0.14044599449732784\n",
      "Iteration: 6000Average reward per game:  -0.11855578719753823\n",
      "Iteration: 6100Average reward per game:  -0.14384292140903074\n",
      "Iteration: 6200Average reward per game:  -0.14127228444230483\n",
      "Iteration: 6300Average reward per game:  -0.12495003558503719\n",
      "Iteration: 6400Average reward per game:  -0.11979568312698533\n",
      "Iteration: 6500Average reward per game:  -0.11641867618148531\n",
      "Iteration: 6600Average reward per game:  -0.1074654997238816\n",
      "Iteration: 6700Average reward per game:  -0.11933576758723703\n",
      "Iteration: 6800Average reward per game:  -0.12372747395725896\n",
      "Iteration: 6900Average reward per game:  -0.10218157307192846\n",
      "Iteration: 7000Average reward per game:  -0.1631373968060802\n",
      "Iteration: 7100Average reward per game:  -0.1490376212350278\n",
      "Iteration: 7200Average reward per game:  -0.1288392222908176\n",
      "Iteration: 7300Average reward per game:  -0.12399737197543496\n",
      "Iteration: 7400Average reward per game:  -0.13226963026976343\n",
      "Iteration: 7500Average reward per game:  -0.13425133720038862\n",
      "Iteration: 7600Average reward per game:  -0.1344929665834041\n",
      "Iteration: 7700Average reward per game:  -0.14877945109943402\n",
      "Iteration: 7800Average reward per game:  -0.13645408326186192\n",
      "Iteration: 7900Average reward per game:  -0.13023845629562067\n",
      "Iteration: 8000Average reward per game:  -0.15520387504311134\n",
      "Iteration: 8100Average reward per game:  -0.11772080887616418\n",
      "Iteration: 8200Average reward per game:  -0.11863509811707262\n",
      "Iteration: 8300Average reward per game:  -0.11073401559053593\n",
      "Iteration: 8400Average reward per game:  -0.14179484823965421\n",
      "Iteration: 8500Average reward per game:  -0.13585596680827058\n",
      "Iteration: 8600Average reward per game:  -0.1375125668513464\n",
      "Iteration: 8700Average reward per game:  -0.16079440278868556\n",
      "Iteration: 8800Average reward per game:  -0.13566137876547785\n",
      "Iteration: 8900Average reward per game:  -0.10024517695126763\n",
      "Iteration: 9000Average reward per game:  -0.12031620886005419\n",
      "Iteration: 9100Average reward per game:  -0.11150979890092276\n",
      "Iteration: 9200Average reward per game:  -0.1232956277204107\n",
      "Iteration: 9300Average reward per game:  -0.10953050483746729\n",
      "Iteration: 9400Average reward per game:  -0.1304342906289637\n",
      "Iteration: 9500Average reward per game:  -0.12928606658449499\n",
      "Iteration: 9600Average reward per game:  -0.11721195423517236\n",
      "Iteration: 9700Average reward per game:  -0.11107839705879721\n",
      "Iteration: 9800Average reward per game:  -0.12772635719416797\n",
      "Iteration: 9900Average reward per game:  -0.11481037025003948\n",
      "Iteration: 10000Average reward per game:  -0.1327701143228249\n",
      "Iteration: 10100Average reward per game:  -0.11263066659733667\n",
      "Iteration: 10200Average reward per game:  -0.11544884834363747\n",
      "Iteration: 10300Average reward per game:  -0.13939214492805865\n",
      "Iteration: 10400Average reward per game:  -0.1398260219080478\n",
      "Iteration: 10500Average reward per game:  -0.11798519685763816\n",
      "Iteration: 10600Average reward per game:  -0.12192154561636817\n",
      "Iteration: 10700Average reward per game:  -0.14636149938198834\n",
      "Iteration: 10800Average reward per game:  -0.14604166874657865\n",
      "Iteration: 10900Average reward per game:  -0.10517931247136174\n",
      "Iteration: 11000Average reward per game:  -0.11957689778425037\n",
      "Iteration: 11096"
     ]
    }
   ],
   "source": [
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "\n",
    "best_acc_val = 0\n",
    "acc_val = 0\n",
    "\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir, now)   \n",
    "\n",
    "n_games_per_update = 10\n",
    "n_max_steps = episode_length\n",
    "n_iterations = 500000\n",
    "save_iterations = 500\n",
    "discount_rate = 0.95\n",
    "\n",
    "summary_freq = 100\n",
    "avg_rewards_list = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    #train_writer = tf.summary.FileWriter(logdir + '/pgtrain', sess.graph)\n",
    "\n",
    "    init.run()\n",
    "    for iteration in range(n_iterations+1):\n",
    "        print(\"\\rIteration: {}\".format(iteration), end=\"\")\n",
    "        all_rewards = []\n",
    "        all_gradients = []\n",
    "        for game in range(n_games_per_update):\n",
    "            current_rewards = []\n",
    "            current_gradients = []\n",
    "            obs = env.reset()\n",
    "            for step in range(n_max_steps):\n",
    "                action_val, gradients_val = sess.run([action, gradients], feed_dict={X: obs.reshape(1, n_inputs)}) \n",
    "                vect_action = int_to_action(action_val,env) #HERE WE CONVERT FROM INTEGER TO ACTION's Array\n",
    "                obs, reward, done, info = env.step(vect_action)\n",
    "                current_rewards.append(reward)\n",
    "                current_gradients.append(gradients_val)\n",
    "                \n",
    "#                 error = sess.run(loss, feed_dict={                                        \n",
    "#                                         X: obs.reshape(1, n_inputs)\n",
    "#                                     })\n",
    "                #print(\"Loss: \", \"Reward: \", reward)\n",
    "                #train_writer.add_summary(summary, iteration)\n",
    "                \n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "            all_rewards.append(current_rewards)\n",
    "            all_gradients.append(current_gradients)\n",
    "            \n",
    "        if iteration % summary_freq == 0:  \n",
    "            avg_rewards = np.mean(np.array(all_rewards))\n",
    "            avg_rewards_list.append(avg_rewards)\n",
    "            \n",
    "            print(\"Average reward per game: \",  avg_rewards / n_games_per_update)    \n",
    "\n",
    "        all_rewards = discount_and_normalize_rewards(all_rewards, discount_rate=discount_rate)\n",
    "        feed_dict = {}\n",
    "        for var_index, gradient_placeholder in enumerate(gradient_placeholders):\n",
    "            mean_gradients = np.mean([reward * all_gradients[game_index][step][var_index]\n",
    "                                      for game_index, rewards in enumerate(all_rewards)\n",
    "                                          for step, reward in enumerate(rewards)], axis=0)\n",
    "            feed_dict[gradient_placeholder] = mean_gradients\n",
    "         \n",
    "       \n",
    "        #tf.summary.scalar('avg_rewards', avg_rewards)\n",
    "\n",
    "\n",
    "        #train_writer.add_summary(avg_rewards, iteration)\n",
    "\n",
    "\n",
    "        sess.run(training_op, feed_dict=feed_dict)\n",
    "        if iteration % save_iterations == 0:\n",
    "                  saver.save(sess, \"./pdenv_policy_net_pg.ckpt\")\n",
    "    #train_writer.close()\n",
    "# with tf.Session() as sess:\n",
    "#         train_writer = tf.summary.FileWriter(log_dir + '/train', sess.graph)\n",
    "#         val_writer = tf.summary.FileWriter(log_dir + '/val')\n",
    "        \n",
    "#         init.run()\n",
    "#         N_TRAIN = X_train.shape[0]\n",
    "#         m = y_train.shape[0]\n",
    "        \n",
    "#         n_batches = int(np.ceil(N_TRAIN/ batch_size))\n",
    "#         #print(n_batches)\n",
    "             \n",
    "#         for epoch in range(epochs+1):\n",
    "            \n",
    "#             for batch_index in range(n_batches):\n",
    "#                 X_batch, y_batch = fetch_batch(X_train,y_train,epoch, batch_index, batch_size, m, n_batches)\n",
    "#                 #print(\"Xbatch shape\",X_batch.shape, \"y_batch shape\", y_batch.shape)\n",
    "#                 sess.run(optimize, feed_dict={\n",
    "#                                     y: y_batch,\n",
    "#                                     X: X_batch\n",
    "#                                 })\n",
    "    \n",
    "#             if summary_freq != None: \n",
    "                            \n",
    "#                 if epoch % summary_freq  == 0:\n",
    "                    \n",
    "\n",
    "#                     summary, acc_train = sess.run([merged, accuracy], feed_dict={\n",
    "#                                         y: y_batch,\n",
    "#                                         X: X_batch\n",
    "#                                     })\n",
    "#                     train_writer.add_summary(summary, epoch)\n",
    "\n",
    "#                     summary, acc_val = sess.run([merged, accuracy], feed_dict={\n",
    "#                                         y: y_val,\n",
    "#                                         X: X_val\n",
    "#                                     }) \n",
    "                                    \n",
    "#                     val_writer.add_summary(summary, epoch)\n",
    "\n",
    "#                     best_acc_val = max(best_acc_val, acc_val)\n",
    "#                     #print(acc_val, best_acc_val)\n",
    "\n",
    "#                     print(\"Epoch: \", epoch, \" Train (batch) accuracy: \", acc_train, \n",
    "#                           \" Validation accuracy: \", acc_val)\n",
    "#                     if best_acc_val <= acc_val:                        \n",
    "#                         save_path = saver.save(sess, model_file)\n",
    "#                         print(\"Saved model with validation accuracy \", acc_val)\n",
    "#         train_writer.close()\n",
    "#         val_writer.close()                            \n",
    "#         #save_path = saver.save(sess, model_file)   \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Iteration: 0Average reward per game:  -131.21281576006612\n",
    "# Iteration: 100Average reward per game:  -92.23713538883702\n",
    "# Iteration: 200Average reward per game:  -70.92879872240424\n",
    "# Iteration: 300Average reward per game:  -46.36746628903998\n",
    "# Iteration: 400Average reward per game:  -43.44814607253152\n",
    "# Iteration: 500Average reward per game:  -26.28340975064497\n",
    "# Iteration: 600Average reward per game:  -24.65947230415822\n",
    "# Iteration: 700Average reward per game:  -30.107651383652463\n",
    "# Iteration: 800Average reward per game:  -17.534408167806813\n",
    "# Iteration: 900Average reward per game:  -19.252239544660405\n",
    "# Iteration: 1000Average reward per game:  -20.431260399119516"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PLAY:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seed = 45\n",
    "np.random.seed(seed)\n",
    "\n",
    "frames = []\n",
    "n_episodes = 5\n",
    "\n",
    "system = PDSystemEnv()\n",
    "\n",
    "model_file = \"pdenv_policy_net_pg.ckpt\"\n",
    "\n",
    "with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        saver.restore(sess, model_file)\n",
    "        for episode in range(n_episodes):\n",
    "            state = env.reset()\n",
    "            for step in range(episode_length):\n",
    "                system.state = state\n",
    "                img = system.visualize()\n",
    "                frames.append(img)\n",
    "\n",
    "                action_val = action.eval(feed_dict={X: state.reshape(1, n_inputs)})\n",
    "                vect_action = int_to_action(action_val,env) #HERE WE CONVERT FROM INTEGER TO ACTION's Array\n",
    "                #print(vect_action)\n",
    "                state, reward, done, info = env.step(vect_action)\n",
    "                #print(state)\n",
    "                #print(action_val[0],emptiest_tank_policy(state, system))\n",
    "        env.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_anim = ut.create_system_animation(frames, n_episodes * episode_length)\n",
    "plt.close()\n",
    "\n",
    "HTML(test_anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.close()\n",
    "\n",
    "#THE REAL LEVELS (percentages 12h, 36h, ? h)\n",
    "tank_levels = [frames[i][2] for i in range(len(frames))]\n",
    "tank_levels_array = np.asarray(tank_levels).transpose()\n",
    "\n",
    "n = system.n\n",
    "\n",
    "cmap = plt.get_cmap('gnuplot')\n",
    "colors = [cmap(i) for i in np.linspace(0, 1, n)]\n",
    "lvl_colors = [\"Orange\", \"Green\",\"Orange\"]\n",
    "\n",
    "tanks_max_load = system.tank_max_loads\n",
    "level_percentages = system.load_level_percentages\n",
    "\n",
    "for i, color in enumerate(colors, start=1):\n",
    "    plt.subplot(3,3, i)    \n",
    "\n",
    "    plt.plot(tank_levels_array[i-1], color=color, label='Tank ${i}$'.format(i=i))\n",
    "    \n",
    "    plt.axhline(y= tanks_max_load[i-1], xmin=0, xmax=episode_length, hold=None, color = \"Red\", linestyle = '--')\n",
    "    for lvl_color, lvl in zip(lvl_colors, level_percentages[i-1]):\n",
    "        plt.axhline(y= lvl * tanks_max_load[i-1], xmin=0, xmax=episode_length, hold=None, color = lvl_color, \n",
    "                    linestyle = '--')\n",
    "    plt.axhline(y= 0, xmin=0, xmax=episode_length, hold=None, color = \"Red\", linestyle = '--')\n",
    "    \n",
    "    \n",
    "    percentages = level_percentages[i-1]           \n",
    "    c = percentages[1]\n",
    "    e = percentages[2]          \n",
    "    d = ct.p0_GLOBAL*e+(1-ct.p0_GLOBAL)*c\n",
    "    plt.axhline(y= d*tanks_max_load[i-1], xmin=0, xmax=episode_length, hold=None, color = \"lawngreen\", \n",
    "                linestyle = '-.')\n",
    "\n",
    "    plt.axhline(y= np.mean(tank_levels_array[i-1]), xmin=0, xmax=episode_length, hold=None, \n",
    "                color = \"blue\", linestyle = '-.')\n",
    "    \n",
    "    plt.legend(loc='best')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "int_to_action(352,env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
