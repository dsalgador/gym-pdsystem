{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dsalgador/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "The algorithm is tested on the PDSystemEnv  gym task \n",
    "and developed with Tensorflow\n",
    "\n",
    "Author: Daniel Salgado Rojo\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gym\n",
    "import gym_pdsystem\n",
    "from gym import wrappers\n",
    "#import tflearn\n",
    "import argparse\n",
    "import pprint as pp\n",
    "\n",
    "#from ddpg.replay_buffer import ReplayBuffer\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import random\n",
    "from gym_pdsystem.envs.pdsystem_env import PDSystemEnv\n",
    "\n",
    "import gym_pdsystem.utils.utilsq as ut\n",
    "import gym_pdsystem.utils.constants as ct\n",
    "\n",
    "\n",
    "from matplotlib import animation, rc\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML\n",
    "\n",
    "import time\n",
    "\n",
    "#TO OMMIT WARNINGS\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# Example n=5, k = 2\n",
    "\n",
    "TANK_MAX_LOADS = np.array([100., 200, 100., 800., 200.])\n",
    "LEVEL_PERCENTAGES = np.array([ #b , c, e\n",
    "                                                [0.02, 0.31, 0.9],\n",
    "                                                [0.01, 0.03, 0.9],\n",
    "                                                [0.05, 0.16, 0.9],\n",
    "                                                [0.07, 0.14, 0.85],\n",
    "                                                [0.08, 0.26, 0.9]\n",
    "                                                   ])\n",
    "\n",
    "TRUCK_MAX_LOADS = np.array([70.,130.])\n",
    "\n",
    "GRAPH_WEIGHTS = np.array([32., 159., 162., 156.,156., 0.])\n",
    "DISCRETE = True\n",
    "############################################################\n",
    "\n",
    "\n",
    "\n",
    "###########################################################\n",
    "# Example n=9, k = 3\n",
    "\n",
    "TANK_MAX_LOADS = np.array([100., 200, 100., 800., 200., 500., 300., 800., 300.])\n",
    "LEVEL_PERCENTAGES = np.array([ #b , c, e\n",
    "                                                [0.02, 0.31, 0.9],\n",
    "                                                [0.01, 0.03, 0.9],\n",
    "                                                [0.05, 0.16, 0.9],\n",
    "                                                [0.07, 0.14, 0.85],\n",
    "                                                [0.08, 0.26, 0.9],\n",
    "                                                [0.02, 0.31, 0.9],\n",
    "                                                [0.01, 0.03, 0.9],\n",
    "                                                [0.05, 0.16, 0.9],\n",
    "                                                [0.07, 0.14, 0.85]\n",
    "                                                   ])\n",
    "\n",
    "TRUCK_MAX_LOADS = np.array([70.,130.,250.])\n",
    "\n",
    "GRAPH_WEIGHTS = np.array([32., 159., 162., 156.,156., 32., 159., 162., 156., 0.])\n",
    "DISCRETE = True\n",
    "############################################################\n",
    "\n",
    "\n",
    "env = gym.make(\"PDSystemEnv-v0\")\n",
    "episode_length = 30\n",
    "env._max_episode_steps = episode_length\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def int_to_action(int_action: int, env):\n",
    "    \"\"\"\n",
    "    So far assumed k = 2 or 3:\n",
    "    \n",
    "    Converts an integer between 0 and env.action_space.shape[1]**env.action_space.shape[0]\n",
    "    which is (n+1)^k where n is the number of tanks and k the number of trucks.\n",
    "    \n",
    "    return vect_action: a k-dimensional vector with components in the range 0,...n. \n",
    "    For k = 2, vect_action = [i,j] is the action of truck 1 going to tank i and truck 2 going to tank j.\n",
    "    (i, j = n means staying at the depot, 0,....,n-1 are the real tanks).\n",
    "    The associated integer is i*(n+1) + j\n",
    "    \n",
    "    For k = 3, vect_action = [i,j,l] is the action of truck 1 going to tank i, truck 2 going to tank j,\n",
    "    and truck 3 going to tank l.\n",
    "    (i, j, l = n means staying at the depot, 0,....,n-1 are the real tanks).\n",
    "    The associated integer is (i*(n+1) + j)*(n+1) + l\n",
    "\n",
    "    \"\"\"\n",
    "    nplus1 = env.action_space.shape[1]\n",
    "    k = env.action_space.shape[0]\n",
    "    n_actions = nplus1**k\n",
    "    \n",
    "    if k == 2:\n",
    "        j = int_action % nplus1\n",
    "        i = int((int_action-j)/nplus1)\n",
    "        vect_action = np.array([i,j])\n",
    "      \n",
    "    elif k == 3:\n",
    "        l = int_action % nplus1\n",
    "        ij = int( (int_action - l)/nplus1 ) \n",
    "        j = ij % nplus1\n",
    "        i = int((ij-j)/nplus1)\n",
    "        vect_action = np.array([i,j,l])\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"The number of trucks k of the environment is different from 2 or 3\")\n",
    "    return vect_action\n",
    "\n",
    "def action_to_int(vect_action: np.array, env):\n",
    "    \"\"\"\n",
    "    Assumed k = 2 or 3, so vect_action has 2 or 3 components\n",
    "    \"\"\"\n",
    "    nplus1 = env.action_space.shape[1]\n",
    "    k = env.action_space.shape[0]\n",
    "    if k == 2:\n",
    "        int_action = vect_action[0] * nplus1 + vect_action[1]\n",
    "    elif k == 3:\n",
    "        int_action = (vect_action[0] * nplus1 + vect_action[1])*nplus1 + vect_action[2]\n",
    "    else:\n",
    "        raise ValueError(\"The number of trucks k of the environment is different from 2 or 3\")\n",
    "\n",
    "    return int_action\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66\n",
      "[0 6 6]\n",
      "66\n"
     ]
    }
   ],
   "source": [
    "int_action = 66\n",
    "print(int_action)\n",
    "vect_action = int_to_action(int_action,env)\n",
    "print(vect_action)\n",
    "int_action = action_to_int(vect_action,env)\n",
    "print(int_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "From https://github.com/ageron/handson-ml \n",
    "\"\"\"\n",
    "\n",
    "def discount_rewards(rewards, discount_rate):\n",
    "    discounted_rewards = np.zeros(len(rewards))\n",
    "    cumulative_rewards = 0\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        cumulative_rewards = rewards[step] + cumulative_rewards * discount_rate\n",
    "        discounted_rewards[step] = cumulative_rewards\n",
    "    return discounted_rewards\n",
    "\n",
    "def discount_and_normalize_rewards(all_rewards, discount_rate):\n",
    "    all_discounted_rewards = [discount_rewards(rewards, discount_rate) for rewards in all_rewards]\n",
    "    flat_rewards = np.concatenate(all_discounted_rewards)\n",
    "    reward_mean = flat_rewards.mean()\n",
    "    reward_std = flat_rewards.std()\n",
    "    return [(discounted_rewards - reward_mean)/reward_std for discounted_rewards in all_discounted_rewards]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/dsalgador/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "# TensorBoard summary directories\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir, now)   \n",
    "\n",
    "\n",
    "seed = 42\n",
    "learning_rate = 0.01 #0.01\n",
    "\n",
    "hidden1_neurons = 300 #100\n",
    "hidden2_neurons = 150 #50\n",
    "\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "            #inputs = tflearn.input_data(shape=[None, self.s_dim])\n",
    "            tf.set_random_seed(seed)\n",
    "            # 1. Parameters to determine the NN architecture\n",
    "\n",
    "            n_inputs = env.observation_space.shape[1]\n",
    "            n_hidden1 = hidden1_neurons; activation1 = tf.nn.sigmoid#tf.nn.elu\n",
    "            n_hidden2 = hidden2_neurons; activation2 = tf.nn.sigmoid#tf.nn.elu\n",
    "            n_outputs = env.action_space.shape[1]**env.action_space.shape[0]\n",
    "            \n",
    "            #indices = [i for i in range(n_outputs)]\n",
    "            \n",
    "            initializer = tf.contrib.layers.variance_scaling_initializer()\n",
    "\n",
    "            # 2. Build the Neural Network\n",
    "            \n",
    "            X = tf.placeholder(tf.float32, shape = (None, n_inputs), name = \"X\")\n",
    "            #y = tf.placeholder(tf.int64, shape = (None), name = \"y\")\n",
    "            \n",
    "            hidden1 = tf.layers.dense(X, n_hidden1, activation = activation1,\n",
    "                                     kernel_initializer = initializer)\n",
    "            hidden2 = tf.layers.dense(hidden1, n_hidden2, activation = activation2,\n",
    "                                     kernel_initializer = initializer)\n",
    "            logits = tf.layers.dense(hidden2, n_outputs)#,kernel_initializer = initializer)\n",
    "            outputs = tf.nn.softmax(logits)\n",
    "                   \n",
    "            \n",
    "with tf.name_scope(\"action\"):\n",
    "            # 3. Select a random action (where to go) based on the estimated probabilities\n",
    "            action = tf.multinomial(tf.log(outputs), num_samples = 1)\n",
    "            #print(tf.rank(action))\n",
    "            #action_onehot = tf.one_hot(indices, n_outputs)\n",
    "            #y = tf.reshape(action_onehot[action], (n_outputs, None))\n",
    "            #y = tf.Variable(action, tf.int64)\n",
    "            y = tf.reshape(action, [1])\n",
    "            #print(tf.rank(y))\n",
    "            #print(tf.rank(logits))\n",
    "            \n",
    "with tf.name_scope(\"loss\"):\n",
    "            xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y,logits = logits)\n",
    "#             xentropy = tf.nn.sigmoid_cross_entropy_with_logits(labels = y,\n",
    "#                                                                       logits = logits)\n",
    "            loss = tf.reduce_mean(xentropy, name = \"loss\")\n",
    "            \n",
    "tf.summary.scalar('average_cross_entropy', loss)\n",
    "\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "            # Optimization Op\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "            #optimize = optimizer.minimize(loss)\n",
    "            \n",
    "            grads_and_vars = optimizer.compute_gradients(xentropy)\n",
    "            gradients = [grad for grad, variable in grads_and_vars]\n",
    "            gradient_placeholders = []\n",
    "            grads_and_vars_feed = []\n",
    "            for grad, variable in grads_and_vars:\n",
    "                gradient_placeholder = tf.placeholder(tf.float32, shape=grad.get_shape())\n",
    "                gradient_placeholders.append(gradient_placeholder)\n",
    "                grads_and_vars_feed.append((gradient_placeholder, variable))\n",
    "            training_op = optimizer.apply_gradients(grads_and_vars_feed)\n",
    "\n",
    "                        \n",
    "# # with tf.name_scope(\"eval\"):\n",
    "# #             correct = tf.nn.in_top_k(logits, y, 1)\n",
    "# #             accuracy = tf.reduce_mean(tf.cast(correct,tf.float32))\n",
    "# # tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "#Extra summary scalars\n",
    "#avg_disc_reward_per_game = tf.placeholder(tf.float32, name = \"avg_disc_reward\")\n",
    "# reward_placeholder = tf.placeholder(tf.float32, name = \"reward_placeholder\")\n",
    "# avg_disc_reward_per_game = tf.Variable(0.0,dtype = tf.float32, name = \"avg_disc_reward\")\n",
    "\n",
    "# tf.summary.scalar(\"Average_Discounted_Reward_per_game\", avg_disc_reward_per_game)\n",
    "                                       \n",
    "merged = tf.summary.merge_all()\n",
    "init = tf.global_variables_initializer()\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
    "        \n",
    "saver = tf.train.Saver()\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0. Average reward per game:  -0.460203 , Elapsed time  0.02  minutes.\n",
      "Iteration: 100. Average reward per game:  -0.223163 , Elapsed time  1.49  minutes.\n",
      "Iteration: 200. Average reward per game:  -0.124124 , Elapsed time  2.93  minutes.\n",
      "Iteration: 300. Average reward per game:  -0.163812 , Elapsed time  4.22  minutes.\n",
      "Iteration: 400. Average reward per game:  -0.132698 , Elapsed time  5.63  minutes.\n",
      "Iteration: 500. Average reward per game:  -0.128192 , Elapsed time  6.97  minutes.\n",
      "Iteration: 600. Average reward per game:  -0.126281 , Elapsed time  8.29  minutes.\n",
      "Iteration: 628"
     ]
    }
   ],
   "source": [
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "\n",
    "\n",
    "# File names for the model\n",
    "model_file = \"./pdenv_policy_net_pg.ckpt\"#.format(learning_rate)\n",
    "graph_file =  '{}.meta'.format(model_file)\n",
    "\n",
    "\n",
    "# Simulation / Training parameters\n",
    "n_games_per_update = 10\n",
    "n_max_steps = episode_length\n",
    "n_iterations = 10**5\n",
    "save_iterations = 100\n",
    "discount_rate = 0.95\n",
    "\n",
    "\n",
    "info_freq = 100\n",
    "round_time = 2\n",
    "round_reward = 6\n",
    "#avg_rewards_list = []\n",
    "\n",
    "summary_freq = 20#int(info_freq/2)\n",
    "\n",
    "\n",
    "retrain = False\n",
    "###########################\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    #train_writer = tf.summary.FileWriter(logdir + '/pgtrain', sess.graph)\n",
    "    if retrain:\n",
    "        saver.restore(sess, model_file)\n",
    "        \n",
    "    summary2 = tf.Summary()\n",
    "    \n",
    "        \n",
    "    init.run()\n",
    "    time_start = time.time()\n",
    "\n",
    "    for iteration in range(n_iterations+1):\n",
    "        print(\"\\rIteration: {}\".format(iteration), end=\"\")\n",
    "        all_rewards = []\n",
    "        all_gradients = []\n",
    "        for game in range(n_games_per_update):\n",
    "            current_rewards = []\n",
    "            current_gradients = []\n",
    "            obs = env.reset()\n",
    "            for step in range(n_max_steps):\n",
    "                action_val, gradients_val = sess.run([action, gradients], feed_dict={X: obs.reshape(1, n_inputs)}) \n",
    "                vect_action = int_to_action(action_val,env) #HERE WE CONVERT FROM INTEGER TO ACTION's Array\n",
    "                obs, reward, done, info = env.step(vect_action)\n",
    "                current_rewards.append(reward)\n",
    "                current_gradients.append(gradients_val)                \n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "            all_rewards.append(current_rewards)\n",
    "            all_gradients.append(current_gradients)\n",
    "            \n",
    "        \n",
    "        all_disc_rewards = discount_and_normalize_rewards(all_rewards, discount_rate=discount_rate)\n",
    "        feed_dict = {}\n",
    "        for var_index, gradient_placeholder in enumerate(gradient_placeholders):\n",
    "            mean_gradients = np.mean([reward * all_gradients[game_index][step][var_index]\n",
    "                                      for game_index, rewards in enumerate(all_disc_rewards)\n",
    "                                          for step, reward in enumerate(rewards)], axis=0)\n",
    "            feed_dict[gradient_placeholder] = mean_gradients\n",
    "         \n",
    "       \n",
    "        #tf.summary.scalar('avg_rewards', avg_rewards)\n",
    "        #train_writer.add_summary(avg_rewards, iteration)\n",
    "\n",
    "        # Summary and info printings     ###################################################################\n",
    "        if iteration % info_freq == 0 or iteration % summary_freq == 0:  \n",
    "            \n",
    "            #avg_rewards =np.array([np.mean(np.array(all_rewards)) / n_games_per_update])\n",
    "            avg_rewards =np.mean(np.array(all_rewards)) / n_games_per_update\n",
    "#             print(avg_rewards)\n",
    "            #avg_rewards_list.append(avg_rewards)\n",
    "            if iteration % summary_freq == 0:\n",
    "                summary2.value.add(tag='average_discounted_reward_per_game', simple_value = avg_rewards)\n",
    "                file_writer.add_summary(summary2, iteration)\n",
    "\n",
    "                feed_dict[X] = obs.reshape(1, n_inputs)\n",
    "                summary = sess.run(merged, feed_dict= feed_dict)\n",
    "\n",
    "                file_writer.add_summary(summary, iteration)\n",
    "                \n",
    "            if iteration % info_freq == 0:\n",
    "                time_end = time.time()\n",
    "                print(\". Average reward per game: \",  round(avg_rewards, round_reward), \n",
    "                      \", Elapsed time \", round( (time_end-time_start)/60., round_time), \" minutes.\")    \n",
    "        ####################################################################################################\n",
    "        \n",
    "        \n",
    "        sess.run(training_op, feed_dict=feed_dict)\n",
    "             \n",
    "        \n",
    "        if iteration % save_iterations == 0:\n",
    "                  saver.save(sess, model_file)\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Iteration: 0Average reward per game:  -131.21281576006612\n",
    "# Iteration: 100Average reward per game:  -92.23713538883702\n",
    "# Iteration: 200Average reward per game:  -70.92879872240424\n",
    "# Iteration: 300Average reward per game:  -46.36746628903998\n",
    "# Iteration: 400Average reward per game:  -43.44814607253152\n",
    "# Iteration: 500Average reward per game:  -26.28340975064497\n",
    "# Iteration: 600Average reward per game:  -24.65947230415822\n",
    "# Iteration: 700Average reward per game:  -30.107651383652463\n",
    "# Iteration: 800Average reward per game:  -17.534408167806813\n",
    "# Iteration: 900Average reward per game:  -19.252239544660405\n",
    "# Iteration: 1000Average reward per game:  -20.431260399119516"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PLAY:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seed = 45\n",
    "np.random.seed(seed)\n",
    "\n",
    "frames = []\n",
    "n_episodes = 5\n",
    "\n",
    "system = PDSystemEnv()\n",
    "\n",
    "model_file = \"pdenv_policy_net_pg.ckpt\"\n",
    "\n",
    "with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        saver.restore(sess, model_file)\n",
    "        for episode in range(n_episodes):\n",
    "            state = env.reset()\n",
    "            for step in range(episode_length):\n",
    "                system.state = state\n",
    "                img = system.visualize()\n",
    "                frames.append(img)\n",
    "\n",
    "                action_val = action.eval(feed_dict={X: state.reshape(1, n_inputs)})\n",
    "                vect_action = int_to_action(action_val,env) #HERE WE CONVERT FROM INTEGER TO ACTION's Array\n",
    "                #print(vect_action)\n",
    "                state, reward, done, info = env.step(vect_action)\n",
    "                #print(state)\n",
    "                #print(action_val[0],emptiest_tank_policy(state, system))\n",
    "        env.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_anim = ut.create_system_animation(frames, n_episodes * episode_length)\n",
    "plt.close()\n",
    "\n",
    "HTML(test_anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.close()\n",
    "\n",
    "#THE REAL LEVELS (percentages 12h, 36h, ? h)\n",
    "tank_levels = [frames[i][2] for i in range(len(frames))]\n",
    "tank_levels_array = np.asarray(tank_levels).transpose()\n",
    "\n",
    "n = system.n\n",
    "\n",
    "cmap = plt.get_cmap('gnuplot')\n",
    "colors = [cmap(i) for i in np.linspace(0, 1, n)]\n",
    "lvl_colors = [\"Orange\", \"Green\",\"Orange\"]\n",
    "\n",
    "tanks_max_load = system.tank_max_loads\n",
    "level_percentages = system.load_level_percentages\n",
    "\n",
    "for i, color in enumerate(colors, start=1):\n",
    "    plt.subplot(3,3, i)    \n",
    "\n",
    "    plt.plot(tank_levels_array[i-1], color=color, label='Tank ${i}$'.format(i=i))\n",
    "    \n",
    "    plt.axhline(y= tanks_max_load[i-1], xmin=0, xmax=episode_length, hold=None, color = \"Red\", linestyle = '--')\n",
    "    for lvl_color, lvl in zip(lvl_colors, level_percentages[i-1]):\n",
    "        plt.axhline(y= lvl * tanks_max_load[i-1], xmin=0, xmax=episode_length, hold=None, color = lvl_color, \n",
    "                    linestyle = '--')\n",
    "    plt.axhline(y= 0, xmin=0, xmax=episode_length, hold=None, color = \"Red\", linestyle = '--')\n",
    "    \n",
    "    \n",
    "    percentages = level_percentages[i-1]           \n",
    "    c = percentages[1]\n",
    "    e = percentages[2]          \n",
    "    d = ct.p0_GLOBAL*e+(1-ct.p0_GLOBAL)*c\n",
    "    plt.axhline(y= d*tanks_max_load[i-1], xmin=0, xmax=episode_length, hold=None, color = \"lawngreen\", \n",
    "                linestyle = '-.')\n",
    "\n",
    "    plt.axhline(y= np.mean(tank_levels_array[i-1]), xmin=0, xmax=episode_length, hold=None, \n",
    "                color = \"blue\", linestyle = '-.')\n",
    "    \n",
    "    plt.legend(loc='best')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "int_to_action(352,env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1 == 2 or 1 == 0:\n",
    "    print(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
